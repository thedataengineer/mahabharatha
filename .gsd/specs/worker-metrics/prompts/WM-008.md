# WM-008: Create integration tests for metrics

## Overview
Create integration tests that verify metrics work end-to-end with real state files.

## Files to Create
- `tests/integration/test_metrics_integration.py`

## Requirements

### Test File Content

```python
"""Integration tests for worker metrics system."""

import json
from datetime import datetime, timedelta
from pathlib import Path

import pytest
from click.testing import CliRunner

from mahabharatha.cli import cli
from mahabharatha.metrics import MetricsCollector
from mahabharatha.state import StateManager
from mahabharatha.types import FeatureMetrics


class TestMetricsPersistence:
    """Tests for metrics persistence to state file."""

    def test_metrics_persist_to_state(self, tmp_path: Path) -> None:
        """Metrics are saved to and loaded from state file."""
        # Create state manager with temp directory
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-persist", state_dir=state_dir)
        state.load()

        # Add some task data
        state.set_task_status("T-001", "complete", worker_id=0)
        state._state["tasks"]["T-001"]["started_at"] = "2026-01-27T10:00:00"
        state._state["tasks"]["T-001"]["completed_at"] = "2026-01-27T10:01:00"
        state._state["tasks"]["T-001"]["duration_ms"] = 60000

        # Compute and store metrics
        collector = MetricsCollector(state)
        metrics = collector.compute_feature_metrics()
        state.store_metrics(metrics)

        # Reload state and verify metrics persisted
        state2 = StateManager("test-persist", state_dir=state_dir)
        state2.load()

        retrieved = state2.get_metrics()

        assert retrieved is not None
        assert retrieved.tasks_completed == 1
        assert isinstance(retrieved, FeatureMetrics)

    def test_metrics_overwrite_on_recompute(self, tmp_path: Path) -> None:
        """Recomputing metrics overwrites previous values."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-overwrite", state_dir=state_dir)
        state.load()

        # First computation
        state.set_task_status("T-001", "complete", worker_id=0)
        collector = MetricsCollector(state)
        state.store_metrics(collector.compute_feature_metrics())

        first_metrics = state.get_metrics()
        assert first_metrics.tasks_completed == 1

        # Add another task and recompute
        state.set_task_status("T-002", "complete", worker_id=0)
        state.store_metrics(collector.compute_feature_metrics())

        second_metrics = state.get_metrics()
        assert second_metrics.tasks_completed == 2


class TestMetricsWithStateManager:
    """Tests for StateManager metrics methods."""

    def test_record_task_claimed(self, tmp_path: Path) -> None:
        """record_task_claimed sets claimed_at timestamp."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-claimed", state_dir=state_dir)
        state.load()

        # Record claim
        state.record_task_claimed("T-001", worker_id=0)

        # Verify claimed_at is set
        task_state = state._state.get("tasks", {}).get("T-001", {})
        assert "claimed_at" in task_state
        assert task_state["worker_id"] == 0

    def test_record_task_duration(self, tmp_path: Path) -> None:
        """record_task_duration sets duration_ms field."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-duration", state_dir=state_dir)
        state.load()

        # Create task first
        state.set_task_status("T-001", "in_progress", worker_id=0)

        # Record duration
        state.record_task_duration("T-001", 45000)

        # Verify duration is set
        task_state = state._state.get("tasks", {}).get("T-001", {})
        assert task_state.get("duration_ms") == 45000


class TestStatusCommandMetrics:
    """Tests for metrics in status command output."""

    def test_json_status_includes_metrics(self, tmp_path: Path) -> None:
        """Status --json output includes metrics section."""
        # Set up state
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-json-metrics", state_dir=state_dir)
        state.load()
        state.set_task_status("T-001", "complete", worker_id=0)
        state._state["tasks"]["T-001"]["duration_ms"] = 30000

        # Compute and store metrics
        collector = MetricsCollector(state)
        state.store_metrics(collector.compute_feature_metrics())

        # Run status command with --json
        runner = CliRunner()
        result = runner.invoke(
            cli,
            ["status", "--feature", "test-json-metrics", "--json"],
            catch_exceptions=False,
        )

        # May fail if state directory isn't found, but if it works:
        if result.exit_code == 0:
            output = json.loads(result.output)
            assert "metrics" in output

    def test_status_displays_without_crash(self, tmp_path: Path) -> None:
        """Status command doesn't crash when displaying metrics."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-no-crash", state_dir=state_dir)
        state.load()

        # Empty state should not crash metrics computation
        collector = MetricsCollector(state)
        metrics = collector.compute_feature_metrics()

        # Verify we got valid metrics (even if empty)
        assert metrics.tasks_total == 0
        assert metrics.workers_used == 0


class TestMetricsAccuracy:
    """Tests for metrics calculation accuracy."""

    def test_level_percentiles_accurate(self, tmp_path: Path) -> None:
        """Level p50/p95 calculations are accurate."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-percentiles", state_dir=state_dir)
        state.load()

        # Add tasks with known durations
        durations = [10000, 20000, 30000, 40000, 50000]
        for i, duration in enumerate(durations):
            task_id = f"T-{i:03d}"
            state.set_task_status(task_id, "complete", worker_id=0)
            state._state["tasks"][task_id]["level"] = 1
            state._state["tasks"][task_id]["duration_ms"] = duration

        state._state["levels"] = {"1": {"status": "complete"}}
        state.save()

        # Compute level metrics
        collector = MetricsCollector(state)
        level_metrics = collector.compute_level_metrics(1)

        # Verify
        assert level_metrics.task_count == 5
        assert level_metrics.completed_count == 5
        assert level_metrics.p50_duration_ms == 30000  # Median
        assert level_metrics.avg_task_duration_ms == 30000.0  # Average

    def test_worker_uptime_calculation(self, tmp_path: Path) -> None:
        """Worker uptime is calculated correctly."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-uptime", state_dir=state_dir)
        state.load()

        # Set worker with started_at in the past
        past = datetime.now() - timedelta(minutes=5)
        state._state["workers"] = {
            "0": {
                "worker_id": 0,
                "status": "ready",
                "started_at": past.isoformat(),
                "ready_at": (past + timedelta(seconds=2)).isoformat(),
                "tasks_completed": 3,
                "context_usage": 0.5,
            }
        }
        state.save()

        # Need to create WorkerState properly for get_worker_state
        from mahabharatha.types import WorkerState
        from mahabharatha.constants import WorkerStatus

        # Mock get_worker_state to return our data
        collector = MetricsCollector(state)
        worker_metrics = collector.compute_worker_metrics(0)

        # Uptime should be roughly 5 minutes (300000ms)
        assert worker_metrics.uptime_ms > 290000  # At least ~4:50
        assert worker_metrics.uptime_ms < 310000  # At most ~5:10


class TestMetricsCleanup:
    """Tests for metrics cleanup and state management."""

    def test_metrics_cleanup_on_delete(self, tmp_path: Path) -> None:
        """State delete removes metrics file."""
        state_dir = tmp_path / ".mahabharatha" / "state"
        state_dir.mkdir(parents=True)

        state = StateManager("test-cleanup", state_dir=state_dir)
        state.load()
        state.save()

        # Verify file exists
        state_file = state_dir / "test-cleanup.json"
        assert state_file.exists()

        # Delete state
        state.delete()

        # Verify file is gone
        assert not state_file.exists()
```

## Verification

```bash
pytest tests/integration/test_metrics_integration.py -v --tb=short
```

## Acceptance Criteria
- [ ] test_metrics_persist_to_state passes
- [ ] test_metrics_overwrite_on_recompute passes
- [ ] test_record_task_claimed passes
- [ ] test_record_task_duration passes
- [ ] test_json_status_includes_metrics passes (or skips gracefully)
- [ ] test_status_displays_without_crash passes
- [ ] test_level_percentiles_accurate passes
- [ ] test_worker_uptime_calculation passes
- [ ] test_metrics_cleanup_on_delete passes
- [ ] Tests use temporary directories
- [ ] Tests clean up after themselves
